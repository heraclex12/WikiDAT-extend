\part{Example cases in Wikipedia data analysis}

\chapter{Activity metrics}

The first example case that we are going to review is the production of overall
activity trends in Wikipedia. Some of these results are similar to those produced
in \url{http://stats.wikimedia.org}, as well as in some well known research
papers, such as[[TODO:ref Voss paper]] and [[TODO:ref Almeida paper]]. This work
is also based on the overall trend statisitics included in my PhD. dissertation
[[TODO:ref Thesis Felipe]].

\section{Questions and goals}
The evolution of the activity registered in Wikipedia has been one of the first
topics of interest for quantitative researchers. As usual, the main spot of
attention has been the English Wikipedia, in particular for creating models
that try to explain and predict the activity trends in this language for the
mid-term (see section~\ref{subsec:overall-further-reading} for more information).

The main goal has been to characterize the evolution of editing activity in
Wikipedia that nurtures its fantastic growth rate (in content and number of
articles). However, over the past 3 years the focus has shifted to explain the
steady-state phase in which it has entered the activity in many larger Wikipedias,
as well as possible reasons that may have influenced this change in the overall
trends.

In the example, we will 

\section{Required data and tools}
In the table \textit{revision} generated by WikiDAT for any language, we can find
the required data to undertake the analysis of overall activity trends. However,
it is a sensible approach to use the information in the \textit{user\_groups} so that
we can make a more accurate analysis by filtering out activity from \textit{bots}
(see section~\ref{sec:routinary-tasks} for more information).

More precisely, we will use the following fields of the \textit{revision} table:

\begin{itemize}
 \item \textit{rev\_id} to count the number of revisions in a given time slot.
 \item \textit{rev\_page} to aggregate scores about number of pages.
 \item \textit{rev\_user} to aggregate scores about users, as well as to elid
information about anonymous users and bots.
 \item \textit{rev\_timestamp} to track scores over time.
 \item \textit{rev\_fa} marking revisions with a FA label in them.
\end{itemize}

Additionally, we can also use information in the \textit{page} table to break
down scores by the namespace of pages, or to calculate the number of
redirects (for articles). In this way, we can obtain for example
how many revisions were received by encyclopedic articles (\textit{main}),
discussion pages (\textit{talk}) and so forth.

\section{Conducting the analysis}
The directory \textit{tools/activity}, in WikiDAT has code to generate 
numerical results and graphics summarizing general
activity metrics at the macroscopic level for any Wikipedia language. 

In the first place, it is necessary to retrieve the information for the 39 different
Wikipedia languages included in the file \textit{data\_august\_2011.py}, using
the parsers as we have explained in Section~\ref{sec:retrieve-store}. Then, we
must prepare our data file in CSV format, running this Python script.

\begin{verbatim}
 jfelipe@blackstorm:~/WikiDAT/sources$ python data_august_2011.py
\end{verbatim}

This will generate the data file \textit{data\_082011.csv}. However, if you want
to directly go ahead and run the R script file, you can use a version of this
file that has been already computed and included in WikiDAT. In this case, the
script computes a series of general descriptive metrics for each Wikipedia
language as for August 2011. We can change the timestamp limits in the queries
to produce results for other months as well (this process will be automated in
forthcoming updates of WikiDAT. Finally you can
load the R script \textit{activity.R} in R studio and click on \textit{Source}
to produce the graphics and numerical results to describe our data. The results
and graphics can be found in the \textit{results} and \textit{figs} folders,
respectively.

% \subsection{Results}

\section{Further reading}
\label{subsec:overall-further-reading}
You can check similar approaches in a number of previous research works. In
particular, Voss in[[TODO:ref measuring Wikipedia]] published in 2005 one of the
first and most inspiring papers on general metrics and trends in Wikipedia.
Almeida et al.[[TODO:Ref]] also studied different metrics to explain the evolution
of Wikipedia. Finally, Chi et al.[[TODO:Ref singularity]] presented a model to
explain the plateau phase in which many activity metrics in the English Wikipedia
have entered. In my PhD. dissertation[[TODO:Ref]] I undertake a broader analysis
spanning the top 10 Wikipedias according to the official article count by 2009.

% \chapter{Distribution of content in articles}
% 
% \section{Questions and goals}
% Text
% 
% \section{Required data and tools}
% Text
% 
% \section{Conducting the analysis}
% Text
% 
% \section{Results}
% Text
% 
% \section{Further reading}
% Text

\chapter{The study of inequalities}

\section{Questions and goals}
Contributions from registered users to Wikipedia are known to be highly unequal,
with a small core of very active editors who perform a high proportion of all 
revisions[[TODO:ref]]. In the 10 largest Wikipedias by number of articles, this
proportion may vary, but it is approximately around 10\% of the total number of
users who made 90\% of all revisions in a given language.

At a smaller scale, the purpose of this example is to explore available tools in
the \textit{ineq} R package to study inequalities in the distribution of a certain
statistic among the members of a population.

\section{Required data and tools}
In the WikiDAT folder \textit{inequality} you will find two data files, \textit{revisions.RData}
and \textit{users.RData} which you can use for experimentation. The R script
\textit{inequalities.R} make use of some tools like the Gini coefficient or the
Lorenz curve to analyze the inequalities in the distributio of revisions made
by wikipedians. In this case, as the examples data files has a small size, the
inequality level is much less extreme that in the case of analyzing the whole
population for one of the big Wikipedias. This is also due to the disproportionately
high number of casual contributors that we find with respect to the short list of
users included in the very active core of editors.

\section{Conducting the analysis}
In this case, you only need to load the file \textit{inequalities.R} in
RStudio and press \textit{Source} to create the numerical results and graphs.

% \subsection{Results}
% Text

\section{Further reading}
In 2008 I published a paper[[TODO:Ref]] with Jesus G. Barahona and G. Robles a paper in the
HICSS conference analyzing the evolution of the Gini coefficient for the top
10 Wikipedias by number of articles at that time. In that study, instead of
evaluating the aggregate inequality level, we studied the inequality per month,
that is, over the total number of contributions per month (instead of over the
total number of contributes in the whole history of each language). In my PhD.
dissertation[[TODO:ref]] I further expand the conclusions of this study. 

% \chapter{Community evolution and generational relay}
% 
% \section{Questions and goals}
% Text
% 
% \section{Required data and tools}
% Text
% 
% \section{Conducting the analysis}
% Text
% 
% \section{Results}
% Text
% 
% \section{Further reading}
Text

\chapter{The study of \textit{logging} actions}

\section{Questions and goals}
Whilst the \textit{pages-meta-history} dump files are by far the most popular of
all Wikipedia database dumps, as we have seen there exist many more alternative
data sources that we can use for our studies. Quite an interesting target can be
the dump file storing all logged actions recorded on the database for every
Wikipedia, including actions such as blocking users, protection of pages, and
other administrative and maintenance tasks.

\section{Required data and tools}
In this case, we parse the dump file for the Simple English Wikipedia (simplewiki)
to demonstrate some of the utilities in R to represent longitudinal data (data points
over time) and build very simple models to analyze trends in these data. No special
tools are required for the analysis in R, since all methods included in the script
come with the standard installation.

\section{Conducting the analysis}
In the first place, we need to parse the logging table for simplewiki, using the
tools provided in WikiDAT. Alternatively, you can download a compressed SQL file
ready to be loaded in MySQL from \url{http://gsyc.es/~jfelipe/WPAC_2012/simplewiki_logging_062012.sql.gz}.
Then, load and execute the R file \textit{logging.R} in the \textit{logging}
directory of WikiDAT. 

% \section{Results}
% Text
% 
% \section{Further reading}
% Text

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TODO: Include many more examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
